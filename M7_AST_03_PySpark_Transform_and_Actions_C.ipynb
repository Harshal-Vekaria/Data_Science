{"cells":[{"cell_type":"markdown","metadata":{"id":"Ps9llghv8jX1"},"source":["### AST 4: PySpark Transform and Actions"]},{"cell_type":"markdown","metadata":{"id":"QeP1PAXf8jYD"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"AkwaW3k58jYG"},"source":["At the end of the experiment, you will be able to\n","\n","* Perform RDD (Resilient Distributed Datasets) operations including:\n","        \n","  1.   Transformations\n","  2.   Actions\n","\n","* Obtain an overview of shuffle operations\n","* Implement RDD based model\n"]},{"cell_type":"markdown","metadata":{"id":"Q3oZtbys8jYL"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"_nR2ZLd-8jYM"},"source":["**Overview about Spark, PySpark and Apache Spark in simple language**\n","\n","**Spark:** A data computational framework that handles Big data.\n","\n","**PySpark:** A tool to support Python with Spark\n","\n","**Apache Spark:** It is an open-source cluster-computing framework, built around speed, ease of use, and streaming analytics.\n","\n","* Like Spark, PySpark helps data scientists to work with (RDDs) Resilient Distributed Datasets. It is also used to work on Data frames. PySpark can be used to work with machine learning algorithms as well.\n","\n","### ***Spark RDD is a major concept in Apache Spark***\n","\n","**Resilient Distributed Datasets:**\n","\n","**Resilient:**    because RDDs are immutable (can’t be modified once created)                        and fault tolerant.\n","\n","**Distributed:**  because it is distributed across clusters\n","\n","**Dataset:**      because it holds data.\n","\n","**Why RDD?**\n","\n","* Apache Spark lets you treat your input files almost like any other variable, which you cannot do in Hadoop MapReduce.\n","* RDDs are automatically distributed across the network by means of Partitions.\n","\n","RDDs are divided into smaller chunks called Partitions, and when you execute some action, a task is launched per partition. This means, the more the number of partitions, the more will be the parallelism.\n","\n","Spark automatically decides the number of partitions that an RDD has to be divided into, but you can also specify the number of partitions when creating an RDD. These partitions of an RDD are distributed across all the nodes in the network.\n","\n","**Difference between Dataframe and RDD (Resilient Distributed Datasets):**\n","\n","**Dataframe:**\n","* Automatically finds out the schema of the dataset.\n","* Performs aggregation faster than RDDs, as it provides an easy API to perform aggregation operations.\n","\n","**RDD:**\n","* We need to define the schema manually.\n","* RDD is slower than Dataframes to perform simple operations like grouping the data.\n"]},{"cell_type":"markdown","metadata":{"id":"t-PfJvpLuTFa"},"source":["**Creating an RDD**\n","\n","**There are three ways to create an RDD in Spark:**\n","1. Parallelizing already existing collection in the driver program.\n","\n","  The key point to note in a parallelized collection is the number of partitions the dataset is divided into. Spark will run one task for each partition of the cluster. We require two to four partitions for each CPU in the cluster. Spark sets the number of partition based on our cluster.\n","\n","2. Referencing a dataset in an external storage system (e.g. HDFS, Hbase, shared file system).\n","  \n","  In Spark, the distributed dataset can be formed from any data source supported by Hadoop, including the local file system, HDFS, Cassandra, HBase etc. In this, the data is loaded from the external dataset.\n","\n","  * csv (String path): It loads a CSV file and returns the result as a Dataset.\n","\n","  * json (String path): It loads a JSON file (one object per line) and returns the result as a Dataset\n","\n","  * textFile (String path) It loads text files and returns a Dataset of String.\n","\n","3. Creating RDD from already existing RDDs.\n","\n","  Transformation mutates one RDD into another RDD, this transformation is the way to create an RDD from an already existing RDD. This creates a difference between Apache Spark and Hadoop MapReduce."]},{"cell_type":"markdown","metadata":{"id":"T0zutSoUD0cs"},"source":["**Actions/Transformations**\n","\n","There are two types of operations that you can perform on an RDD-\n","* Transformations\n","* Actions.\n","\n","**Transformation** applies some function on an RDD and creates a new RDD, it does not modify the RDD that you apply the function on. Also, the new RDD keeps a pointer to its parent RDD.\n","\n","When you call a transformation, Spark does not execute it immediately, instead it creates a lineage. A lineage keeps track of what all transformations have to be applied on that RDD, including from where it has to read the data.\n","\n","\n","**Action** is used to either save the result to some location or to display it. You can also print the RDD lineage information by using the command:\n","\n","\"filtered.toDebugString\" -> (*filtered* is the RDD here)."]},{"cell_type":"markdown","metadata":{"id":"9Ja7Mz2TMhIR"},"source":["<img style=\"-webkit-user-select: none;margin: auto;\" src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Pyspark_RDD.JPG\" width=\"500\" height=\"400\">\n"]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"markdown","metadata":{"id":"6Q9PCT0kBVB3"},"source":[" **Install PySpark**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Xzay908G5qQ"},"outputs":[],"source":["!pip install pyspark"]},{"cell_type":"markdown","metadata":{"id":"BobYAePRT6Ok"},"source":["**Creating Spark Session**\n","\n","Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0 (Instead of having various contexts, everything is encapsulated in a Spark session)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-tj32cQHmBb"},"outputs":[],"source":["# Start spark session\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, udf  # User Defined Functions\n","from pyspark.sql.types import StringType\n","spark = SparkSession.builder.appName('Rdd').getOrCreate()\n","spark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypfIQnQczPMy"},"outputs":[],"source":["# Accessing sparkContext from sparkSession instance.\n","sc = spark.sparkContext"]},{"cell_type":"markdown","metadata":{"id":"5Kt4YX3C3dGT"},"source":["### Spark Python Transformations"]},{"cell_type":"markdown","metadata":{"id":"LuBu_zaiYWX-"},"source":["**map()** - A map transformation is useful when we need to transform an RDD by applying a function to each element."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vJJS0NS_o8y"},"outputs":[],"source":["# Return a new RDD by applying a function to each element of this RDD.\n","rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n","sorted(rdd.map(lambda x: (x, 1)).collect())"]},{"cell_type":"markdown","metadata":{"id":"rRmlweKeDvjf"},"source":["**take()** - Take the first num elements of the RDD.\n","\n","It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGskOAX2_s0x"},"outputs":[],"source":["sc.parallelize([2, 3, 4, 5, 6]).cache().take(2) #take()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6oZr8ukWkHAW"},"outputs":[],"source":["sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3) #take()"]},{"cell_type":"markdown","metadata":{"id":"h3fEehJeEguG"},"source":["**flatMap()** - The flatMap transformation will return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. This is the main difference between the flatMap and *map transformations.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAelIgUTf3Sy"},"outputs":[],"source":["s0 = sc.parallelize([3,4,5])\n","s0.flatMap(lambda x: [x, x*x]).collect()"]},{"cell_type":"markdown","metadata":{"id":"tJLRChBQxnmG"},"source":["Compare the same function using map()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBJegE2cxmAq"},"outputs":[],"source":["sc.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect()"]},{"cell_type":"markdown","metadata":{"id":"WihTRM55aQzK"},"source":["**filter()** - The filter transformation returns a new dataset formed by selecting  those elements of the source on which func returns true."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgypZMSQGyR_"},"outputs":[],"source":["rdd = sc.parallelize([1, 2, 3, 4, 5])\n","rdd.filter(lambda x: x % 2 == 0).collect() # Return a new RDD containing only the elements that satisfy a predicate."]},{"cell_type":"markdown","metadata":{"id":"-FV3bYIKJi31"},"source":["**groupByKey()** - We can apply the “groupByKey” transformations on (key,val) pair RDD. The “groupByKey” will group the values for each key in the original RDD. It will create a new pair, where the original key corresponds to this collected group of values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uyh3Z6mDOMbM"},"outputs":[],"source":["x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n","x.groupByKey().map(lambda x : (x[0], list(x[1]))).collect()"]},{"cell_type":"markdown","metadata":{"id":"MjbYWjBCLfUn"},"source":["**reduceByKey()** - Merge the values for each key using an associative reduce function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_kSwLv6OmzV"},"outputs":[],"source":["from operator import add\n","rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n","sorted(rdd.reduceByKey(add).collect())"]},{"cell_type":"markdown","metadata":{"id":"UV-_pmJVLfl5"},"source":["**mapPartitions()** - Is similar to map, but runs separately on each partition (block) of the RDD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2UtmrsS5uA0"},"outputs":[],"source":["wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n","\n","wordsRDD = sc.parallelize(wordsList, 4) # number of partitions - 4\n","\n","print(wordsRDD.collect())\n","\n","itemsRDD = wordsRDD.mapPartitions(lambda iterator: [','.join(iterator)])\n","# mapPartitions() loops through 4 partitions and combines('rat,cat') in 4th iteration.\n","print (itemsRDD.collect())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sBZnXOHiHMjU"},"outputs":[],"source":["L = range(1,10)\n","\n","parallel = sc.parallelize(L, 3) # number of partitions - 3\n","\n","def f(iterator):\n","  yield sum(iterator)\n","\n","parallel.mapPartitions(f).collect()\n","\n","# Results [6,15,24] are created because mapPartitions() loops through 3 partitions, Partion 1: 1+2+3 = 6, Partition 2: 4+5+6 = 15, Partition 3: 7+8+9 = 24\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rclESZfdRbMM"},"outputs":[],"source":["rdd = sc.parallelize([1, 2, 3, 4], 2) # number of partitions - 2\n","\n","def f(iterator):\n","  yield sum(iterator)\n","\n","rdd.mapPartitions(f).collect()\n","\n","# Results [3, 7], partition 1 : 1+2 = 3, partition 2 : 3+4 =7"]},{"cell_type":"markdown","metadata":{"id":"B-SUV2VcJjMJ"},"source":["**mapPartitionsWithIndex()** - Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7dgV6f21Us_m"},"outputs":[],"source":["rdd = sc.parallelize([1, 2, 3, 4], 4)\n","def f(splitIndex, iterator): yield splitIndex\n","rdd.mapPartitionsWithIndex(f).sum()"]},{"cell_type":"markdown","metadata":{"id":"82fgwxs8kzy5"},"source":["### Spark Python Actions"]},{"cell_type":"markdown","metadata":{"id":"qaYgbjhklcT0"},"source":["**Creating an RDD to explain \"RDD actions with Examples\"**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"seJ35vrGleKN"},"outputs":[],"source":["data=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n","\n","inputRDD = spark.sparkContext.parallelize(data)\n","\n","listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])\n","\n","from operator import add"]},{"cell_type":"markdown","metadata":{"id":"HTRyVfftnLaD"},"source":["After creating two RDDs as given above, we use these two as and when necessary to demonstrate the RDD actions."]},{"cell_type":"markdown","metadata":{"id":"Bjcr2K9fk0LT"},"source":["**first()** – Return the first element in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR-hsPBqnds7"},"outputs":[],"source":["#first\n","print(\"first :  \"+str(listRdd.first()))\n","print(\"first :  \"+str(inputRDD.first()))"]},{"cell_type":"markdown","metadata":{"id":"CFIF21pnk0tc"},"source":["**take()** – Return the first num elements of the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8iyLrZ0nopK"},"outputs":[],"source":["#take()\n","print(\"take : \"+str(listRdd.take(2)))"]},{"cell_type":"markdown","metadata":{"id":"a5dG25M-k02C"},"source":["**takeSample()** – Return the subset of the dataset in an Array."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J58_ENwkn15A"},"outputs":[],"source":["print(\"take : \"+str(listRdd.takeSample(0,3))) # ([1,2,3,4,5,3,2])"]},{"cell_type":"markdown","metadata":{"id":"XddOZ8LRk08Y"},"source":["**takeOrdered()** – Return the first num (smallest) elements from the dataset and this is the opposite of the take() action."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nptmLS_qoSKt"},"outputs":[],"source":["print(\"takeOrdered : \"+ str(listRdd.takeOrdered(2)))"]},{"cell_type":"markdown","metadata":{"id":"-chsPGWPk1EQ"},"source":["**collect()** - Return the complete dataset as an Array."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMDnTnA0pNHG"},"outputs":[],"source":["#Collect\n","data = listRdd.collect()\n","print(data)"]},{"cell_type":"markdown","metadata":{"id":"r1How8QYk1NK"},"source":["**count()** – Return the count of elements in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5YNtF_RpW6y"},"outputs":[],"source":["print(\"Count : \"+str(listRdd.count()))"]},{"cell_type":"markdown","metadata":{"id":"sNSjv_LCpcAU"},"source":["**countByValue()** – Return Map[T,Long] key representing each unique value in dataset and value represents count each value present."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ij3GMKfbpcLs"},"outputs":[],"source":["print(\"countByValue :  \"+str(listRdd.countByValue()))"]},{"cell_type":"markdown","metadata":{"id":"qNHqoneIplsk"},"source":["**reduce()** – Reduces the elements of the dataset using the specified binary operator."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"McRegvjapl0T"},"outputs":[],"source":["redRes=listRdd.reduce(add)\n","print(redRes)"]},{"cell_type":"markdown","metadata":{"id":"yklH98ELqV_q"},"source":["**top()** – Return top n elements from the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pj1f2tsCqWWj"},"outputs":[],"source":["print(\"top : \"+str(listRdd.top(2)))\n","print(\"top : \"+str(inputRDD.top(2)))"]},{"cell_type":"markdown","metadata":{"id":"OONqU-CTtEkZ"},"source":["**fold()** - Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral \"zero value.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"euxrIYOYtFEu"},"outputs":[],"source":["foldRes=listRdd.fold(0, add)\n","print(foldRes)"]},{"cell_type":"markdown","metadata":{"id":"khV8HhGc0lhx"},"source":["**foldByKey()** -  is quite similar to fold(), both use a zero value of the same type of the data in our RDD and combination function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aGQdKuM0L9n"},"outputs":[],"source":["inputRDD.foldByKey(0, add).collect()"]},{"cell_type":"markdown","metadata":{"id":"ehIGu2bG0-kc"},"source":["**reduceByKey()** - Merge the values for each key using an associative reduce function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PXKBSyi0-zi"},"outputs":[],"source":["sorted(inputRDD.reduceByKey(add).collect())"]},{"cell_type":"markdown","metadata":{"id":"-6S5yGzv13Ck"},"source":["**combineByKey()** - Generic function to combine the elements for each key using a custom set of aggregation functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSpVQHCL13Ls"},"outputs":[],"source":["def f(inputRDD):\n","  return inputRDD\n","def add(A, B):\n","  return A + str(B)\n","sorted(inputRDD.combineByKey(str, add, add).collect())"]},{"cell_type":"markdown","metadata":{"id":"PFXAbayxXKtw"},"source":["### PySpark User Defined Functions"]},{"cell_type":"markdown","metadata":{"id":"BXEXD6LUXFu7"},"source":["* PySpark UDF is a User Defined Function that is used to create a reusable\n","function in Spark.\n","\n","* Once UDF is created, that can be re-used on multiple DataFrames and SQL (after registering).\n","\n","* The default type of the udf() is StringType."]},{"cell_type":"markdown","metadata":{"id":"PCyKMMvCGRkk"},"source":["Created dataframe with two columns \"Seqno\" and \"Name\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvCmOPMTDtZv"},"outputs":[],"source":["columns = [\"Seqno\",\"Name\"]\n","data = [(\"1\", \"john jones\"),\n","    (\"2\", \"tracey smith\"),\n","    (\"3\", \"amy sanders\")]\n","\n","df = spark.createDataFrame(data=data,schema=columns)\n","\n","df.show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"lon2qElWGW8Z"},"source":["Applying UDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9TQBD5JZTh3"},"outputs":[],"source":["# creating a udf using lambda\n","convertUDF = udf(lambda z: z.upper())\n","df.select(col(\"Seqno\"), convertUDF(col(\"Name\")).alias(\"Name\") ).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"fiEIyKJ4xmKj"},"source":["#### **Shuffle Operations**\n"]},{"cell_type":"markdown","metadata":{"id":"-XP9I8tYbl_i"},"source":["Shuffling is a mechanism PySpark uses to redistribute the data across different executors and even across machines. PySpark shuffling triggers when we perform certain transformation operations like gropByKey(), reduceByKey(), join() on RDDS\n","\n","Spark also supports transformations with wide dependencies, such as groupByKey and reduceByKey. In these dependencies, the data required to compute the records in a single partition can reside in many partitions of the parent dataset.\n","\n","To perform these transformations, all of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy this requirement, Spark performs a shuffle, which transfers data around the cluster and results in a new stage with a new set of partitions.\n","\n","For example, consider the following code:\n","\n","**sc.textFile(\"someFile.txt\").map(mapFunc).flatMap(flatMapFunc).filter(filterFunc).count()**\n","\n","It runs a single action, count, which depends on a sequence of three transformations on a dataset derived from a text file. This code runs in a single stage because none of the outputs of these three transformations depend on data that comes from different partitions than their inputs."]},{"cell_type":"markdown","metadata":{"id":"huYvlG3IAq_A"},"source":["**Below is an example implementing RDD based model to count the words given in a file**"]},{"cell_type":"markdown","metadata":{"id":"-b3HdfYQjLe-"},"source":["\n","\n","To implement RDD based model, we have used the text file (**Spark_Text.txt**) which includes Apache Spark notes/information. This text file contains 5 paragraphs of information on Spark.\n","\n","We would perform RDD Transformations and Actions on the file to count the words given in the text file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1WbnApmkBle"},"outputs":[],"source":["rdd = sc.textFile(\"Spark_Text.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gpTlIzzYH3V"},"outputs":[],"source":["# To lower the case of each word of a document, we can use the map transformation.\n","def Func(lines):\n","      lines = lines.lower()\n","      lines = lines.split()\n","      return lines\n","rdd1 = rdd.map(Func)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-h8E_vJiplH"},"outputs":[],"source":["rdd1.take(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3zknNLjYH-d"},"outputs":[],"source":["#To get the flat output, we need to apply a transformation which will flatten the output, The transformation “flatMap\" will help here:\n","rdd2 = rdd.flatMap(Func)\n","rdd2.take(3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q0wfA1urYIB0"},"outputs":[],"source":["rdd3 = rdd2.filter(lambda x:x!= '')\n","rdd3.take(7)  # We can check first 7 elements of “rdd3” by applying take action."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0c9lxazfYIFM"},"outputs":[],"source":["rdd3_mapped = rdd3.map(lambda x: (x,1))\n","rdd3_grouped = rdd3_mapped.groupByKey()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIozl8hjYV0P"},"outputs":[],"source":["rdd3_mapped.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).sortByKey(False).take(200)"]},{"cell_type":"markdown","metadata":{"id":"RTbilp60aHcl"},"source":["**In the below example we can see Spark Transformations in Python using a CSV file.**"]},{"cell_type":"markdown","metadata":{"id":"OqdCmWOejijF"},"source":["We will use this CSV file (**Google_Books.csv**) to work on Spark Transformations.\n","\n","This data was acquired from the Google Books store. Google API was used to acquire the data. Nine features were gathered for each book in the data set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P357wQsFb5XS"},"outputs":[],"source":["book_names = sc.textFile(\"google_books.csv\")\n","rows = book_names.map(lambda line: line.split(\",\")) #we are creating a new RDD called “rows” by splitting every row in the book_names RDD."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0wNy70CdRpC"},"outputs":[],"source":["for row in rows.take(rows.count()):\n","  print(row[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n01Rlq6wee8X"},"outputs":[],"source":["for row in rows.take(10):\n","  print(row[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvdfQzEXjDq1"},"outputs":[],"source":["# filter() - Creating a new RDD by returning only the elements that satisfy the search filter.\n","rows.filter(lambda line: \"Inward Journey\" in line).collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKguXKc1lAtF"},"outputs":[],"source":["# groupByKey() The following groups all titles to their publisher. Operates on value pairs\n","rows = book_names.map(lambda line: line.split(\",\"))\n","titleToPublisher = rows.map(lambda n: (str(n[0]),str(n[6]) )).groupByKey()\n","titleToPublisher.map(lambda x : {x[0]: list(x[1])}).take(5)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}