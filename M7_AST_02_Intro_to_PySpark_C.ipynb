{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"1NClk94Yt7bLrW0IAH4XdMj87Zc66o98h","timestamp":1617886040526}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Nr6L0U0ahbzr"},"source":["### AST 2: Intro to PySpark"]},{"cell_type":"markdown","metadata":{"id":"hJ-wAYVIhbzu"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"vnSM9v-nhbzv"},"source":["At the end of the experiment, you will be able to\n","\n","* interact with Spark using python\n","* understand Spark dataframes\n","* implement linear regression using PySpark"]},{"cell_type":"markdown","metadata":{"id":"O3T1Tj2fhbzw"},"source":["### Dataset"]},{"cell_type":"markdown","metadata":{"id":"m8LVD5gchbzw"},"source":["The dataset chosen for this assignment is [Ecommerce customers](https://www.kaggle.com/srolka/ecommerce-customers). The dataset is made up of 500 records and 8 columns. It has customer information, such as e-mail, address, and their color avatar. Then it also has numerical value columns.\n","\n","* Avg Session Length: Average session of in-store style advice sessions\n","* Time on App: Average time spent on App in minutes\n","* Time on Website: Average time spent on Website in minutes\n","* Length of Membership: How many years the customer has been a member.\n","* Yearly Amount Spent\n","\n","Here, we will be using the first four features to perform linear regression using spark and predict Yearly Amount Spent by each customer."]},{"cell_type":"markdown","metadata":{"id":"jS6X9xQQhbzx"},"source":["### Information"]},{"cell_type":"markdown","metadata":{"id":"FLUYa2jkhbzx"},"source":["**Why do we need Spark?**\n","\n","Spark is one of the latest technologies being used to quickly and easily handle Big Data. Spark is an open-source distributed computing framework that promises a clean and pleasurable experience similar to that of Pandas, while scaling to large data sets via a distributed architecture under the hood.\n","\n","Apache Spark is a powerful cluster computing engine, therefore it is designed for fast computation of big data. Spark runs on Memory (RAM), and that makes the processing much faster than on Disk. It includes \"MLlib\" library to perform Machine Learning tasks using the Spark framework."]},{"cell_type":"markdown","metadata":{"id":"yP0oxBKKhbzx"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"b-MIetu-hbzy"},"source":["Apache Spark is known as a fast, easy to use and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing. It’s well-known for its speed, ease of use, generality and the ability to run virtually everywhere. And even though Spark is one of the most asked tools for data engineers, also data scientists can benefit from Spark when doing exploratory data analysis, feature extraction, supervised learning and model evaluation.\n","\n","Spark is a platform for cluster computing that lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n","\n","As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster."]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"markdown","metadata":{"id":"dxCv5teXYANs"},"source":["### Importing required packages"]},{"cell_type":"code","metadata":{"id":"ZB8vMNkUYANt"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4dwQCqpwhbzy"},"source":["### PySpark"]},{"cell_type":"markdown","metadata":{"id":"GYVoE0nThbzz"},"source":["PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment. PySpark supports most of Spark’s features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core.\n","\n","<figure>\n","<img src='https://cdn.iisc.talentsprint.com/CDS/Images/pyspark_components.png' width = 700 px/>\n","</figure>\n","\n","**Spark SQL and DataFrame**\n","\n","Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrame and can also act as distributed SQL query engine.\n","\n","**Streaming**\n","\n","Running on top of Spark, the streaming feature in Apache Spark enables powerful interactive and analytical applications across both streaming and historical data, while inheriting Spark’s ease of use and fault tolerance characteristics.\n","\n","**MLlib**\n","\n","Built on top of Spark, MLlib is a scalable machine learning library that provides a uniform set of high-level APIs that help users create and tune practical machine learning pipelines.\n","\n","**Spark Core**\n","\n","Spark Core is the underlying general execution engine for the Spark platform that all other functionality is built on top of. It provides an RDD (Resilient Distributed Dataset) and in-memory computing capabilities."]},{"cell_type":"markdown","metadata":{"id":"tuVyxDSAhbzz"},"source":["#### Install PySpark"]},{"cell_type":"code","metadata":{"id":"dpbyJFSZhbzz"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1-JqQmojhbz0"},"source":["#### Start a Spark Session"]},{"cell_type":"markdown","metadata":{"id":"Vju8S9ZWhbz0"},"source":["Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0. It provides a way to interact with various spark’s functionality with a lesser number of constructs. Instead of having spark context, hive context, SQL context, now everything is encapsulated in a Spark session."]},{"cell_type":"code","metadata":{"id":"bdlUo9HFhbz0"},"source":["# Start spark session\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('LinearRegression').getOrCreate()\n","spark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ryDcIBTYhbz1"},"source":["### Data Processing using Pyspark"]},{"cell_type":"markdown","metadata":{"id":"qfm30-_ghbz1"},"source":["#### Loading data into PySpark"]},{"cell_type":"markdown","metadata":{"id":"2YyXV9pthbz1"},"source":["To load the dataset we will use the `read.csv` module.  The `inferSchema` parameter provided will enable Spark to automatically determine the data type for each column. Also, `header` and `sep` parameters are given as the dataset contains header, and values are separated using vertical bar."]},{"cell_type":"code","metadata":{"id":"5g1oX-k0hbz2"},"source":["df = spark.read.csv(\"ecommerce_customers_.csv\", sep = \"|\", header=True, inferSchema = True)           # creating spark data frame"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fe2TqI_7hbz3"},"source":["#### Data exploration with PySpark"]},{"cell_type":"markdown","metadata":{"id":"6MOtesQghbz3"},"source":["* Display data types of dataframe columns"]},{"cell_type":"code","metadata":{"id":"z_xCliuMhbz4"},"source":["# Print the data types\n","df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EfpmyItPhbz4"},"source":["* Display column details"]},{"cell_type":"code","metadata":{"id":"KOnGCTXZhbz4"},"source":["# Print the Schema of the DataFrame\n","df.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRI0Hi6phbz5"},"source":["* Display rows"]},{"cell_type":"code","metadata":{"id":"B4pNnsfwhbz5"},"source":["df.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OicqJUYfhbz5"},"source":["* Display total number of rows"]},{"cell_type":"code","metadata":{"id":"iVMYaotJhbz5"},"source":["df.count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJLKs8i3hbz6"},"source":["* Display column labels"]},{"cell_type":"code","metadata":{"id":"0CbPm_VUhbz6"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tbp-QZ3fhbz6"},"source":["* Display specific columns"]},{"cell_type":"code","metadata":{"id":"uicyXLDAhbz6"},"source":["columns = [\"Email\",\"Time on App\",\"Time on Website\"]\n","df.select(columns).show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zyJyBM-Chbz6"},"source":["* Display the statistics of dataframe"]},{"cell_type":"code","metadata":{"id":"qEcVV2Achbz7"},"source":["df.describe().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S7ncz4X2hbz7"},"source":["* Display total distinct values in *Avatar* column"]},{"cell_type":"code","metadata":{"id":"u3IUdO7Ohbz7"},"source":["# Distinct value count\n","df.select('Avatar').distinct().count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tDwuxwbfhbz8"},"source":["* Display count of distinct values in *Avatar* column"]},{"cell_type":"code","metadata":{"id":"up-ef7XNhbz8"},"source":["df.groupby('Avatar').count().show(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5bUrhKkhbz8"},"source":["* Plot the count of distinct values in *Avatar* column"]},{"cell_type":"code","metadata":{"id":"fY39dnmohbz8"},"source":["DF = df.groupby('Avatar').count().sort(\"count\", ascending= False)\n","DF.show(8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uy5htH2Rhbz9"},"source":["plt.figure(figsize= (24,4))\n","x = DF.toPandas()['Avatar']\n","y = DF.toPandas()['count']\n","sns.barplot(x=x, y=y)\n","plt.xticks(rotation= 90)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WwFMYWqKhbz9"},"source":["* Display average time spent on app by users having different *Avatar*"]},{"cell_type":"code","metadata":{"id":"2Wi64y0Qhbz9"},"source":["df.groupby('Avatar').avg().select(['Avatar', 'avg(Time on App)']).show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dcCKpLhYhbz-"},"source":["* Display the records where average time spent on website by user is greater than 37 minutes"]},{"cell_type":"code","metadata":{"id":"nLD3pmpQhbz-"},"source":["df.filter(df['Time on Website'] > 37).show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nB82Y8vhbz-"},"source":["* Display the minimum Yearly Amount Spent where average time spent on website by user is greater than 39 minutes"]},{"cell_type":"code","metadata":{"id":"V7caT8Lvhbz_"},"source":["from pyspark.sql.functions import col, min\n","df.filter(col('Time on Website')>39).agg(min('Yearly Amount Spent')).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S1hUIVRAhbz_"},"source":["* Display the records where average time spent on app by user is greater than 12 minutes and average time spent on website is smaller than 37 minutes"]},{"cell_type":"code","metadata":{"id":"tJ-Hib-Bhbz_"},"source":["from pyspark.sql.functions import col\n","df.filter((col('Time on App')>12) &(col('Time on Website') < 37)).show(10, truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LORVLYUMhb0A"},"source":["To know more about other `pyspark.sql.functions` operation click [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)."]},{"cell_type":"markdown","metadata":{"id":"qI55eNtchb0A"},"source":["### Linear Regression Model"]},{"cell_type":"markdown","metadata":{"id":"0SQI5Rq3hb0A"},"source":["Linear Regression model is one of the oldest and widely used machine learning approach which assumes a relationship between dependent and independent variables. It consists of the best fitting line through the scattered points on the graph and this best fitting line is known as the regression line."]},{"cell_type":"markdown","metadata":{"id":"Ij78EczBhb0B"},"source":["#### Setting Up DataFrame for Model"]},{"cell_type":"markdown","metadata":{"id":"exKHqPx4hb0B"},"source":["For Spark to accept the data, it needs to be in the form of two columns (\"labels\", \"features\")\n","\n","* Features are data points of all the attributes to be used for prediction\n","* Labels are output for each data point\n","* We will be predicting Label from Features"]},{"cell_type":"markdown","metadata":{"id":"aCZCS-R4hb0B"},"source":["For the linear regression model, we need to import two modules from Pyspark i.e. Vector Assembler and Linear Regression. Vector Assembler is a transformer that assembles all the features into one vector from multiple columns that contain type double.\n","\n","To know more about vector assembler click [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html)."]},{"cell_type":"code","metadata":{"id":"3gHmgn46hb0C"},"source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.regression import LinearRegression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S26ufGiZhb0C"},"source":["assembler = VectorAssembler(\n","                            inputCols= [\"Avg Session Length\", \"Time on App\", \"Time on Website\",'Length of Membership'],\n","                            outputCol= \"features\")       # features is the name of output columns which combines all the columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i9umLjr7hb0C"},"source":["output = assembler.transform(df)            # A new column 'features' will be created along with the existing columns\n","                                            # features column will include all the values combined in one list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WOxDfY4uhb0D"},"source":["output.show(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5l3IvdYNhb0D"},"source":["output.select(\"features\").show(10, truncate= False)          # displays only the features column (which includes all other column values in a list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MsMl8o7Yhb0E"},"source":["# Complete dataset is represented in 2 columns\n","final_data = output.select(\"features\",'Yearly Amount Spent')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8dyT5lLehb0E"},"source":["#### Splitting the data into Training and Test set"]},{"cell_type":"code","metadata":{"id":"cznY4s_Ahb0F"},"source":["# Splitting the data in Train and Test set(70% training data, 30% testing data)\n","train_data,test_data = final_data.randomSplit([0.7,0.3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fQzau_Pvhb0F"},"source":["train_data.describe().show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrdTvsmVhb0F"},"source":["test_data.describe().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LW62eB6Hhb0G"},"source":["#### Create a Linear Regression Model object and fit on train data"]},{"cell_type":"code","metadata":{"id":"86V5LekMhb0G"},"source":["regressor = LinearRegression(featuresCol=\"features\", labelCol=\"Yearly Amount Spent\")\n","\n","#Learn to fit the model from training set\n","model = regressor.fit(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YVH4X80nhb0G"},"source":["#### Predicting the Test set results"]},{"cell_type":"code","metadata":{"id":"DCrfunGbhb0G"},"source":["predict = model.transform(test_data)\n","\n","predict.select(predict.columns[:]).show(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHlcdOtThb0G"},"source":["#### Evaluating Model Performance"]},{"cell_type":"code","metadata":{"id":"41ftFH8Ohb0H"},"source":["metrics = model.evaluate(test_data)                             # Using evaluate method we can verify our model's performance\n","\n","print('Mean absolute error: {}'.format(metrics.meanAbsoluteError))\n","print('Root mean squared error: {}'.format(metrics.rootMeanSquaredError))\n","print('R_squared value: {}'.format(metrics.r2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VcCSeWV3hb0H"},"source":["To know more about other operations in pyspark click [here](https://cdn.iisc.talentsprint.com/CDS/cheatSheet_pyspark.pdf)."]}]}