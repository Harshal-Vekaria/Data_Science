{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"NCtEIcCVFhO8"},"source":["### AST 4: ETL concepts and pipeline"]},{"cell_type":"markdown","metadata":{"id":"nH6llaZpFhPC"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"qxQatabcFhPD"},"source":["At the end of the experiment, you will be able to:\n","\n","* use Sparkâ€™s built-in and external data sources to read, refine, and write data in different file formats as part of the extract, transform, and load (ETL) tasks\n","* perform complex data exploration and analysis using Spark SQL"]},{"cell_type":"markdown","metadata":{"id":"GAEv9h69FhPE"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"nFzjN683FhPE"},"source":["ETL (Extract, Transform, and Load) is the procedure of migrating data from one system to another.\n","* Data **extraction** is the process of retrieving data out of homogeneous or heterogeneous sources for further data processing and data storage.\n","* During data **transformation**, the data is cleaned and incorrect or inaccurate records are modified or deleted.\n","* Finally, the processed data is **loaded** (or stored) into a target system such as a data warehouse or NoSQL database or RDBMS.\n","\n","Data engineers use Spark because it provides a simple way to parallelize computations and hides all the complexity of distribution and fault tolerance. This leaves them free to focus on using high-level DataFrame-based APIs and domain-specific language queries to do ETL, reading and combining data from multiple sources.\n","\n","Here we will consider tabular data to do ETL operations. Starting from data extraction, we will perform various transformations and try to gain some insights from it and then load it to a NoSQL database or store it in different file formats."]},{"cell_type":"markdown","metadata":{"id":"jptEWxkKFhPD"},"source":["### Dataset"]},{"cell_type":"markdown","metadata":{"id":"1b0GRwC0FhPE"},"source":["The dataset chosen for this assignment is [Productivity Prediction of Garment Employees](https://archive.ics.uci.edu/ml/datasets/Productivity+Prediction+of+Garment+Employees). The dataset is made up of 1197 records and 15 columns. It includes important attributes of the garment manufacturing process and the productivity of the employees. The dataset contains records of three months (Jan to Mar 2015) with 2 distinct departments, 12 unique team numbers and 5 different quarters. Some of the features are listed below:\n","\n","* date: Date in MM-DD-YYYY\n","* day: Day of the Week\n","* quarter: A portion of the month. A month was divided into four quarters\n","* department: Associated department with the instance\n","* team: Associated team number with the instance\n","* no_of_workers: Number of workers in each team\n","* no_of_style_change: Number of changes in the style of a particular product\n","* targeted_productivity: Targeted productivity set by the Authority for each team for each day.\n","* smv: Standard Minute Value, it is the allocated time for a task\n","* wip: Work in progress. Includes the number of unfinished items for products\n","* overtime: Represents the amount of overtime by each team in minutes\n","* incentive: Represents the amount of financial incentive (in BDT) that enables or motivates a particular course of action.\n","* idletime: The amount of time when the production was interrupted due to several reasons\n","* idlemen: The number of workers who were idle due to production interruption\n","* actual_productivity: The actual % of productivity that was delivered by the workers. It ranges from 0-1.\n","\n","Date, quarter, department, and day are object datatypes and the rest are int or float types.\n","\n","To know more about the dataset click [here](https://archive.ics.uci.edu/ml/datasets/Productivity+Prediction+of+Garment+Employees)."]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"markdown","metadata":{"id":"dHY77Q8DFhPF"},"source":["### Install Pyspark"]},{"cell_type":"code","metadata":{"id":"KPf_EpH-FhPF"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cq_ToATpFhPF"},"source":["### Import required packages"]},{"cell_type":"code","metadata":{"id":"FmMx7T9eFhPG"},"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.feature import MinMaxScaler\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bO5nb-O0FhPG"},"source":["### Start a Spark Session"]},{"cell_type":"markdown","metadata":{"id":"pRSLW_ZqFhPG"},"source":["Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0. Instead of having various context, everything is now encapsulated in a **Spark session**."]},{"cell_type":"code","metadata":{"id":"sY9Ht3RFFhPG"},"source":["# Start spark session\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('ETL').getOrCreate()\n","spark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gOYuB7SBFhPG"},"source":["### Tabular Data Analytics"]},{"cell_type":"markdown","metadata":{"id":"MBlN05cNFhPH"},"source":["#### Extract data into PySpark\n","To load the dataset we will use the read.csv module.  The inferSchema parameter provided will enable Spark to automatically determine the data type for each column."]},{"cell_type":"code","metadata":{"id":"cHiMKSPZFhPH"},"source":["df = spark.read.csv('garments_worker_productivity.csv', header=True, inferSchema= True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ni5SQb1uFhPH"},"source":["#### Transforming Data"]},{"cell_type":"markdown","metadata":{"id":"8hOkkyXNFhPI"},"source":["* Display first few rows of the data"]},{"cell_type":"code","metadata":{"id":"PqiEA5SqFhPI"},"source":["df.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eyu_0_uHFhPI"},"source":["In the above output, the wip (work in progress) column contains null values. We need to check for other columns as well.\n","* Display total number of rows"]},{"cell_type":"code","metadata":{"id":"6-49cC7GFhPI"},"source":["df.count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZKWUDmyyFhPJ"},"source":["* Check for Null values in each column"]},{"cell_type":"code","metadata":{"id":"PYQZZSAZFhPJ"},"source":["df.select([(count(when(isnan(c) | col(c).isNull(), c))/1197).alias(c) for c in df.columns]).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wocbg9GvFhPK"},"source":["The wip column contains around 42% null values so we can drop that column.\n","* Drop the wip column having Null values"]},{"cell_type":"code","metadata":{"id":"VdYpNTTAFhPK"},"source":["df1 = df.drop('wip')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ig3e4w9HFhPK"},"source":["# Recheck for null values\n","df1.select([(count(when(isnan(c) | col(c).isNull(), c))/1197).alias(c) for c in df1.columns]).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5M_8GG9FhPL"},"source":["Let's see the distinct values in department column.\n","* Display distinct `department` from dataframe"]},{"cell_type":"code","metadata":{"id":"PhXnYf4OFhPL"},"source":["# Display count of distinct 'department'\n","df1.select('department').distinct().count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSXZe3pwFhPL"},"source":["# Display distinct values for 'department' column\n","df1.select('department').distinct().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I68uWOsiFhPL"},"source":["There is ambiguity in distinct department values and it needs to be taken care of.\n","* Transform department column"]},{"cell_type":"code","metadata":{"id":"yZdsPCaAFhPL"},"source":["# Removing trailing spaces from both sides using department column\n","df2 = df1.withColumn('department', trim(col('department')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oRyZMZlSFhPM"},"source":["# Display distinct values for 'department' column\n","df2.select('department').distinct().show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_195IjXEFhPM"},"source":["# Replacing department value from 'sweing' to 'sewing'\n","df3 = df2.withColumn('department', regexp_replace(col('department'), 'sweing', 'sewing'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T2b72zb6FhPM"},"source":["# Display distinct values for 'department' column\n","df3.select('department').distinct().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5YPfkb-yFhPM"},"source":["* Check for duplicate values in data"]},{"cell_type":"code","metadata":{"id":"kJHpOnaBFhPM"},"source":["cols = df3.columns\n","if df3.count() > df3.dropDuplicates(cols).count():\n","    print('Data has duplicates')\n","else:\n","  print('Data has no duplicates')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-91dTPIBFhPN"},"source":["If data has duplicate values, run the below cell by uncommenting it"]},{"cell_type":"code","metadata":{"id":"eS5t2r0_FhPN"},"source":["# df3 = df3.dropDuplicates(df3.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hFFNpfG-FhPN"},"source":["After removing duplicates, let's take a look at the datatypes of our columns.\n","* Display data types of dataframe columns"]},{"cell_type":"code","metadata":{"id":"fdaA7KR4FhPN"},"source":["# Print the data types\n","df3.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CvLXXC_BFhPN"},"source":["Here, the 'date' column has a string datatype. In order to perform analytics involving year and month, we need to convert it into timestamp datatype.\n","* Transform the `date` column from string type to Spark `timestamp` data type"]},{"cell_type":"code","metadata":{"id":"X8PVJt_bFhPO"},"source":["df4 = df3.withColumn(\"date\", to_timestamp(col(\"date\"), \"M/d/yyyy\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"19alU-8kFhPO"},"source":["df4.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Clxysl2cFhPO"},"source":["Now we can use the `year()` SQL Spark function on the Timestamp column data type `date`.\n","* Display how many distinct years of data is in the dataset"]},{"cell_type":"code","metadata":{"id":"Zqe9JtGpFhPO"},"source":["df4.select(year('date')).distinct().orderBy(year('date')).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRbxYZF1FhPO"},"source":["We see that all the records are from the year 2015.\n","\n","Similar to year() we can use the `month()` SQL Spark function on the Timestamp datatype column `date`.\n","* Display how many distinct months of data is in the dataset"]},{"cell_type":"code","metadata":{"id":"ISePnPs0FhPR"},"source":["df4.select(month('date')).distinct().orderBy(month('date')).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rS8SPyPGFhPS"},"source":["We see that the data was collected in the months January to March."]},{"cell_type":"markdown","metadata":{"id":"C_ZMvuN9FhPS"},"source":["* Check in which month the productivity was maximum"]},{"cell_type":"code","metadata":{"id":"F5fc05-0FhPS"},"source":["df_d = df4.groupby(month('date')).avg().select(['month(date)', 'avg(actual_productivity)'])\n","df_d.show()\n","sns.barplot(x = df_d.toPandas()['month(date)'], y= df_d.toPandas()['avg(actual_productivity)'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nSgxRTs2FhPS"},"source":["From the above plot, it can be seen that in January the productivity was little higher than remaining two.\n","\n","Let's gain few more insights from the data\n","* Display the incentives paid to different teams"]},{"cell_type":"code","metadata":{"id":"1GM_CVgZFhPS"},"source":["df_i = df4.groupby('team').avg().select(['team', 'avg(incentive)'])\n","df_i.show()\n","sns.barplot(x = df_i.toPandas()['team'], y= df_i.toPandas()['avg(incentive)'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9IRv_tDkFhPS"},"source":["So on average team 9 received the highest incentive.\n","* Display number of workers in each Team"]},{"cell_type":"code","metadata":{"id":"9_3wTe13FhPS"},"source":["df_w = df4.groupby('team').sum().select(['team', 'sum(no_of_workers)'])\n","df_w.show()\n","sns.barplot(x = df_w.toPandas()['team'], y= df_w.toPandas()['sum(no_of_workers)'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9EHlIhavFhPT"},"source":["From the above plot, it can be seen that team 6 and 12 have less number of workers compared to other teams.\n","\n","Let's find out how much it costs for the first quarter of the first month.\n","* Display the `incentive` paid for the first quarter of the first month"]},{"cell_type":"code","metadata":{"id":"KIKPLKx1FhPT"},"source":["df_q = df4.select(month(\"date\"), \"quarter\", \"incentive\").where((col('month(date)') == 1) & (col(\"quarter\") == \"Quarter1\"))\n","df_q.show(5)\n","df_q.groupby('quarter').sum().select('sum(incentive)').show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1oXWp_fFhPT"},"source":["Before fitting to a model, the outlier removal and feature scaling of data are important.\n","* Check for outliers"]},{"cell_type":"code","metadata":{"id":"N35RLI5PFhPT"},"source":["df4.toPandas().boxplot()\n","plt.xticks(rotation= 90)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4arjqqo6FhPT"},"source":["* Handling outliers\n","\n","Here instead of removing the outliers we will change their values to upper bound and lower bound depending on whether the value is higher than upper bound or lower than lower bound respectively."]},{"cell_type":"code","metadata":{"id":"NkOMFKlvFhPT"},"source":["df5 = df4\n","outlier_cols = [\"targeted_productivity\", \"smv\", \"over_time\", \"incentive\", \"idle_time\", \"idle_men\", \"no_of_style_change\", \"actual_productivity\"] # Columns with outliers\n","def handle_outliers(df, colm):\n","    df = df.toPandas()\n","    q1 = df.describe()[colm].loc[\"25%\"]\n","    q3 = df.describe()[colm].loc[\"75%\"]\n","    iqr = q3 - q1\n","    lower_bound = q1 - (1.5 * iqr)\n","    upper_bound = q3 + (1.5 * iqr)\n","    for i in range(len(df)):\n","      if df.loc[i,colm] > upper_bound:\n","        df.loc[i,colm]= upper_bound\n","      if df.loc[i,colm] < lower_bound:\n","        df.loc[i,colm]= lower_bound\n","    return spark.createDataFrame(df)\n","\n","for colm in outlier_cols:\n","    df5 = handle_outliers(df5, colm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWlcasCMFhPT"},"source":["# Recheck for outliers\n","df5.toPandas().boxplot()\n","plt.xticks(rotation= 90)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DnDUZeBPFhPU"},"source":["* Scaling features"]},{"cell_type":"code","metadata":{"id":"xpZy1k0GFhPU"},"source":["# Display the statistics of dataframe\n","df5.toPandas().describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZaHW1C7FhPU"},"source":["# Specify columns that need to be scaled\n","columns = [\"smv\", \"over_time\", \"incentive\", \"no_of_workers\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8pgGe4yFhPU"},"source":["print(\"Before Scaling :\")\n","df5.show(5)\n","df6 = df5\n","\n","from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.sql.functions import udf, round\n","from pyspark.sql.types import DoubleType\n","\n","# UDF for converting column type from vector to double type\n","unlist = udf(lambda x: float(list(x)[0]), DoubleType())\n","spark.udf.register(\"unlist\", unlist)\n","\n","# Iterating over columns to be scaled\n","for i in columns:\n","    # VectorAssembler Transformation - Converting column to vector type\n","    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n","\n","    # MinMaxScaler Transformation\n","    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n","\n","    # Pipeline of VectorAssembler and MinMaxScaler\n","    pipeline = Pipeline(stages=[assembler, scaler])\n","\n","    # Fitting pipeline on dataframe\n","    df6 = pipeline.fit(df6).transform(df6).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n","\n","print(\"After Scaling :\")\n","df6.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XrNKGgMLFhPU"},"source":["Let's create a new `id` column that will contain a unique value for each record."]},{"cell_type":"code","metadata":{"id":"Vws1d6rPFhPU"},"source":["df7 = df6.withColumn(\"id\", monotonically_increasing_id()+1)\n","df7.select('id').show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AxtWPtpBFhPU"},"source":["We can also perform SQL queries on spark dataframe using the `spark.sql()` function. But for that, we first need to register the dataframe as a table in the spark catalog. We can do this using the `createOrReplaceTempView()` spark dataframe method. It takes the name of the temporary table we'd like to register as argument. As this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark dataframe."]},{"cell_type":"code","metadata":{"id":"IQuN6nz5FhPV"},"source":["df7.createOrReplaceTempView('df_table')\n","spark.sql(\"select date, department, team, smv, over_time, incentive from df_table where department='sewing'\").show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NuxGK5lMFhPV"},"source":["#### Load Data"]},{"cell_type":"markdown","metadata":{"id":"WZa9WGtjFhPV"},"source":["Once we have extracted and transformed our data, we might want to load it into the destination or store it somewhere. We will load it into the MongoDB database."]},{"cell_type":"markdown","metadata":{"id":"Cw_To9V-FhPW"},"source":["Data in MongoDB is represented and stored using JSON-style documents. In PyMongo we use dictionaries to represent documents."]},{"cell_type":"code","metadata":{"id":"YQtw9vFnFhPW"},"source":["data = df7.toPandas()\n","documents = []\n","for i in range(len(data)):\n","        doc = data.iloc[i,:].to_dict()\n","        for keys in doc:\n","          if keys != 'date':\n","            if type(doc[keys]) not in [str]:\n","                doc[keys] = float(doc[keys])\n","        documents.append(doc)\n","documents[0:1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gBdnqWBmFhPW"},"source":["If you would like to perform the data insertion step then please **create your own account** on MongoDB Atlas as given in the reference [here](https://cdn.iisc.talentsprint.com/CDS/DB_Connect_Docs/Assignment_MongoDB_Connect.pdf) and change the credentials and run the below code by uncommenting it."]},{"cell_type":"code","metadata":{"id":"y4JC5Zg5FhPX"},"source":["### new_document = coll.insert_many(documents)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ayplTl--FhPX"},"source":["Also, we can store the data in other formats like json, csv, and parquet and read it back whenever required.\n","\n","**Store the dataframe as a `json file`**"]},{"cell_type":"code","metadata":{"id":"jGbjYqmDFhPX"},"source":["df7.write.format(\"json\").mode(\"overwrite\").save('transformed_json_data.json')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9hzX8LdjFhPX"},"source":["**Read data from `json` to spark dataframe**"]},{"cell_type":"code","metadata":{"id":"p2--YPdhFhPY"},"source":["df_json = spark.read.format(\"json\").load('transformed_json_data.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJVFRRoQFhPY"},"source":["df_json.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3_5wx0aWFhPY"},"source":["**Store the dataframe as a `csv file`**"]},{"cell_type":"code","metadata":{"id":"mpABXzByFhPY"},"source":["df7.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"transformed_csv_data.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7mO6h-HpFhPY"},"source":["**Read data from `csv` to spark dataframe**"]},{"cell_type":"code","metadata":{"id":"-oDKXltXFhPY"},"source":["df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").load('transformed_csv_data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sW53Urb8FhPY"},"source":["df_csv.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"is1lmuQuFhPY"},"source":["**Use Parquet files to store data**"]},{"cell_type":"markdown","metadata":{"id":"3NfAZB2cFhPZ"},"source":["Parquet uses snappy compression to compress the data. If the DataFrame is written as Parquet, the schema is preserved as part of the Parquet metadata.\n","\n","To know more about parquet file format click [here](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module5/ellicium_com_blog_parquet_file_format_structure_teevel_2C_20the_stored_20in_20the_20footer_20section.pdf)."]},{"cell_type":"code","metadata":{"id":"vQ-u4QOMFhPZ"},"source":["df7.write.format(\"parquet\").mode(\"overwrite\").save(\"transformed_parquet_data\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hSnBkk5KFhPZ"},"source":["**Read data from Parquet file**\n","\n","We don't have to specify the schema here since it's stored as part of the Parquet metadata."]},{"cell_type":"code","metadata":{"id":"lqsho9lhFhPZ"},"source":["df_parquet = spark.read.format(\"parquet\").load(\"transformed_parquet_data\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1MQ19ypFhPZ"},"source":["df_parquet.show(5)"],"execution_count":null,"outputs":[]}]}