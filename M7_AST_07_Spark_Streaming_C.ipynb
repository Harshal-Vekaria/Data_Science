{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KAVRGjLgyrot"},"source":["### AST 7: Spark Streaming\n"]},{"cell_type":"markdown","metadata":{"id":"JJBuWr-J0ZNJ"},"source":["## Learning Objectives\n","At the end of the experiment, you will be able to :\n","\n","* understand Spark Structured Streaming pipeline and implement it using Pyspark.\n","* understand Structured Streaming Query along with steps involved."]},{"cell_type":"markdown","metadata":{"id":"U5IsqgiwxW1h"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"nQ10vyRp4fhd"},"source":["### Streaming data:\n","Streaming data is data that is continuously generated by different sources such as applications, networking devices, server log files, website activity, banking transactions and location data. Such data should be processed incrementally using Stream Processing techniques without having access to all of the data. Besides, it should be considered that concept drift may happen in the data which means that the properties of the stream may change over time.\n","It is usually used in the context of big data in which it is generated by many different sources at high speed.\n","\n","To know more about streaming data click [here](https://aws.amazon.com/streaming-data/).\n"]},{"cell_type":"markdown","metadata":{"id":"L12kUAyW74CK"},"source":["### Examples of streaming data\n","* IoT/Sensor generated data at certain time intervals in vehicles, industrial equipments, and farm machineries can be monitored to check performance and detect defects in advance.\n","* Continuous changing stock market value can be tracked for the financial institution in real-time.\n","* A media publisher streams billions of clickstream records from its online properties, aggregates and enriches the data with demographic information about users, and optimizes content placement on its site, delivering relevancy and a better experience to its audience."]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"markdown","metadata":{"id":"WMe_jHKRBi8T"},"source":["#### Install Pyspark"]},{"cell_type":"code","metadata":{"id":"bo5xZ80qAwQ0"},"source":["!pip -qq install pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Bw3m0z1losx"},"source":["#### Importing required packages"]},{"cell_type":"code","metadata":{"id":"sbAf4Cu7lk73"},"source":["import os\n","import time\n","import random\n","from pyspark.sql import SparkSession\n","import pyspark.sql.functions as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ncjCyzUFBmlf"},"source":["#### Starting the Spark Session"]},{"cell_type":"code","metadata":{"id":"_ois4kAQU89j"},"source":["spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wVUB5vsR_EiZ"},"source":["#### Reading Data\n","\n"]},{"cell_type":"code","metadata":{"id":"FnUT-3FEatZi"},"source":["df=spark.read.csv('TempHumi.csv',header=True,inferSchema=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4Ut9upybYGQ"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mq9bp7Kgbm2H"},"source":["df.show(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m8J09KewDEY4"},"source":["df.select('Date').dtypes # Checking data type of any column"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wc-a1pflbp_r"},"source":["df.groupBy(\"Hour\").count().show(24)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x6VnY2qTdz16"},"source":["We can now save the output of that job by filtering on each step and saving it to a separate file"]},{"cell_type":"code","metadata":{"id":"hGGl8q4ThLcG"},"source":["!mkdir HourFolder  # Making a folder on current directory"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hn32hkt4fSr3"},"source":["steps = df.select(\"Hour\").distinct().collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w5f4NTkAXcQ-"},"source":["print(steps)\n","print(type(steps))\n","print(steps[0])\n","print(steps[0][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4x3lRhUFgF6t"},"source":["df_test = df.where(f\"Hour={steps[0][0]}\")\n","df_test.show()\n","type(df_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZUFWUP_D3878"},"source":["###  Creating the streaming version of this input\n","Implementing the above steps in the loop and making individual `csv` files for each hour step. These files are saved in a folder as given below. We will use this folder as a source of incoming stream data and, we will read each file one by one as if it is a stream.\n","\n","To know more about coalesce, click [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html)"]},{"cell_type":"code","metadata":{"id":"1QGslwXV0DSB"},"source":["for step in steps:\n","  df1 = df.where(f\"Hour={step[0]}\")\n","  df1.coalesce(1).write.csv(path='/content/HourFolder/Hourly_Record',header=\"true\",mode=\"append\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QeVV2nslQ__z"},"source":["Hourly_Record_list = [i for i in os.listdir(\"/content/HourFolder/Hourly_Record/\") if i.endswith(\".csv\")]\n","Hourly_Record_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z814p9LmAKa-"},"source":["#### Checking any one file from above :"]},{"cell_type":"code","metadata":{"id":"EFrec3uA7ouY"},"source":["file_name = Hourly_Record_list[random.randint(0,len(Hourly_Record_list)-1)]\n","file_name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9_ER-lJFi0XG"},"source":["part =spark.read.csv(\"/content/HourFolder/Hourly_Record/\"+file_name, header=True,inferSchema=True)\n","part.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRXFTqiokc1u"},"source":["### The Programming Model of Structured Streaming\n","“Table” is a well-known concept that developers are familiar with when building\n","batch applications. Structured Streaming extends this concept to streaming applications by treating a stream as an unbounded, continuously appended table, as illustrated in Figure.\n","\n","![data_stream_unbounded%20table.PNG](https://cdn.iisc.talentsprint.com/CDS/Images/data_stream_unbounded_table.PNG)"]},{"cell_type":"markdown","metadata":{"id":"HDjm6ltt7oua"},"source":["Every new record received in the data stream is like a new row being appended to the unbounded input table. This leads to a new stream processing model that is very similar to a batch processing model. We will express our streaming computation as a standard batch-like query as on a static table, and Spark runs it as an incremental query on the unbounded input table. Let’s understand this model in more detail. A query on the input will generate the “Result Table” as given in the figure below. Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table. Whenever the result table gets updated, we would want to write the changed result rows to an external sink.\n","\n","![spark%20stream.PNG](https://cdn.iisc.talentsprint.com/CDS/Images/spark_stream.PNG)\n","\n","The “Output” is defined as what gets written out to the external storage. The output can be defined in a different mode:\n","\n","Complete Mode - The entire updated Result Table will be written to the external storage. It is up to the storage connector to decide how to handle the writing of the entire table.\n","\n","Append Mode - Only the new rows appended in the Result Table since the last trigger will be written to the external storage. This is applicable only on the queries where existing rows in the Result Table are not expected to change.\n","\n","Update Mode - Only the rows that were updated in the Result Table since the last trigger will be written to the external storage (available since Spark 2.1.1). Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger. If the query doesn’t contain aggregations, it will be equivalent to Append mode.\n","\n","Note that each mode is applicable on certain types of queries."]},{"cell_type":"markdown","metadata":{"id":"o9phYdr9AwM_"},"source":["### Five Steps to Define a Streaming Query\n","We have prepared our data as if it is coming from a continuous streaming source now in this section, we will explore the steps involved in defining a streaming query.\n","#### Step 1: Define input sources\n","*  The first step is to define a DataFrame from a streaming source. For streaming sources, we need spark .readStream to create a DataStreamReader.\n","* Apache Spark natively supports reading data streams from Apache Kafka and all the various file-based formats that DataFrameReader supports (Parquet, ORC, JSON, etc.).\n","* We can think of the folder containing 24 csv files as the Kafka source feeding to spark streaming."]},{"cell_type":"code","metadata":{"id":"8_JfVHJSjCX9"},"source":["part.schema # checking the part schema"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKYaLoD9mIDF"},"source":["streaming = (spark.readStream.schema(part.schema).option('maxFilesPerTrigger',1).csv('HourFolder/Hourly_Record/'))\n","# maxFilesPerTrigger: maximum number of new files to be considered in every trigger, here taken 1.\n","# File source - Reads files written in a directory as a stream of data, here directory: HourFolder/Hourly_Record.\n","# Supported file formats are text, CSV, JSON, ORC, Parquet, here is CSV.\n","# Files will be processed in the order of file modification time."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUbu5CQAfh2u"},"source":["type(streaming)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ih6DFcBnfnD"},"source":["#### Step 2: Setting  Up Transformation:\n"]},{"cell_type":"code","metadata":{"id":"f42sTO_4npM4"},"source":["Hourly_Mean_Value = streaming.groupBy('Hour').mean(\"Temperatur (C)\",\"Humidity (%)\").orderBy(F.desc(\"Hour\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"48W9a1n0ol1d"},"source":["* Hourly_Mean_Value is a streaming DataFrame (that is, a DataFrame on unbounded, streaming data) that represents the running mean that will be computed once the streaming query is started and the streaming input data is being continuously processed.\n","* Now that we have our transformation, we need to specify an output sink for the results. for this example, we are going to write to a memory sink that keeps the results in memory.\n","* We also need to define how spark will output that data. In this example, we will use the complete output mode (rewriting all of the keys along with their counts after every trigger).\n","* In this example, we will not include activity query.awaitTermination() because it is required only to prevent the driver process from terminating when the stream is active. So to be able to run this locally in a notebook we will not include it."]},{"cell_type":"markdown","metadata":{"id":"llFK0nBFK8rC"},"source":["#### Step 3: Define output sink and output mode"]},{"cell_type":"code","metadata":{"id":"OEQZ1SGDpn8Q"},"source":["writer = (Hourly_Mean_Value.writeStream.queryName(\"Temp_Humi_Mean\").format(\"memory\").outputMode(\"complete\").start())\n","# Memory sink (for debugging) - The output is stored in memory as an in-memory table.\n","# Both, Append and Complete output modes, are supported.\n","# This should be used for debugging purposes on low data volumes.\n","# as the entire output is collected and stored in the driver’s memory. Hence, use it with caution.\n","# Have all the aggregates in an in-memory table. The query name will be the table name, here: \"Temp_Humi_Mean\".\n","# Note that we have to call start() to start the execution of the query.\n","# This returns a StreamingQuery object which is a handle to the continuously running execution."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zGGaFb7eO2Oq"},"source":["#### Step 4: Specifying processing details:\n"," * Triggering details: This indicates when to trigger the discovery and processing of newly available streaming data.\n"," * Checkpoint location: This option is necessary for failure recovery in the real application.\n","\n"," For sake of simplicity, these parameters are not set by us here. By default trigger --> The streaming query executes data in micro-batches where the next micro-batch is triggered as soon as the previous micro-batch has been completed.\n"]},{"cell_type":"markdown","metadata":{"id":"rnoOEDe1Q2op"},"source":["#### Step 5: Start the query :\n","* Once everything has been specified, the final step is to start the query, already done in the above cell by .start(). We have created a table with --> .queryName (\"Temp_Humi_Mean\") which is updated according to trigger.\n","* This is a StreamingQuery object which is a handle to the continuously running execution. We can use this object to manage the query."]},{"cell_type":"code","metadata":{"id":"mLpnql1xrGJN"},"source":["for x in range(45):\n","  print('Query Result at time step : ',x)\n","  df_q = spark.sql(\"SELECT * FROM Temp_Humi_Mean\")\n","  df_q.show(24)\n","  time.sleep(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkDWUi1ArnUc"},"source":["#### Check if the stream is active"]},{"cell_type":"code","metadata":{"id":"XwkUTvYzT8ST"},"source":["spark.streams.active  # get the list of currently active streaming queries."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9dZB5N3rrGP"},"source":["spark.streams.active[0].isActive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ck30USBBrrMC"},"source":["writer.status\n","# It gives information about what the query is immediately doing - is a trigger active, is data being processed, etc.\n","# Will print something like the following."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WjliUvd_sHIm"},"source":["If we want to turn off the stream we'll run writer.stop() to reset the query for testing purposes."]},{"cell_type":"code","metadata":{"id":"Vg84yA8rsGji"},"source":["writer.stop()  # stop the query."],"execution_count":null,"outputs":[]}]}