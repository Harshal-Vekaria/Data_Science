{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"LM_GMDQ-J1uN"},"source":["### AST 5: ETL pipeline for Text Mining and Analytics"]},{"cell_type":"markdown","metadata":{"id":"pAZ01Z5tJ1uV"},"source":["At the end of the experiment, you will be able to:\n","\n","* perform text mining and analytics using Spark SQL functions\n","* use Spark’s built-in and external data sources to write data in different file formats as part of the extract, transform, and load (ETL) tasks\n"]},{"cell_type":"markdown","metadata":{"id":"J93wmCVyJ1uW"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"MyHY_53qJ1uX"},"source":["The basic terminology related to text analytics are\n","\n","* **Text**: a sequence of words and punctuation\n","* **Corpus**: a large body of text\n","* **Frequency distribution**: the frequency of words in a text object\n","* **Collocation**: a sequence of words that occur together unusually often\n","* **Bigrams**: word pairs. High frequent bigrams are collocations\n","* **Text normalization**: the process of transforming text into a single canonical form, e.g., converting text to lowercase, removing punctuations and stop words."]},{"cell_type":"markdown","metadata":{"id":"UexUWZb2J1uX"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"F2dNzmmEJ1uX"},"source":["Text analytics is the process of deriving information from text. It usually involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging, information extraction, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP), different types of algorithms and analytical methods.\n","\n","Here we will consider `milton-paradise.txt` text file from Gutenberg corpus to do text mining and analytics. Starting from data extraction, we will perform various transformations on text including tokenization, the number of words counting, POS tagging, chunking and then store it in different file formats."]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"markdown","metadata":{"id":"GsODE6yLJ1uY"},"source":["### Install Pyspark"]},{"cell_type":"code","metadata":{"id":"PbVweILTJ1uY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737185158012,"user_tz":-330,"elapsed":2995,"user":{"displayName":"Debasish Bhaskar","userId":"07671455864892136061"}},"outputId":"2bf836eb-8bb8-4b1f-ea58-9fc8c14dcc3f"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"]}]},{"cell_type":"markdown","metadata":{"id":"gjdtWmUaJ1uY"},"source":["### Import required packages"]},{"cell_type":"code","metadata":{"id":"i2qaeOmyJ1uZ"},"source":["from pyspark.sql import SparkSession\n","from matplotlib import pyplot as plt\n","import pandas as pd\n","import string\n","from nltk import Tree\n","from pyspark.ml.feature import NGram\n","from pyspark.ml import Pipeline\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f8ZWzA-VJ1uZ"},"source":["### Start a Spark Session"]},{"cell_type":"markdown","metadata":{"id":"RKKkyz0IJ1uZ"},"source":["Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0. Instead of having various context, everything is now encapsulated in a Spark session."]},{"cell_type":"code","metadata":{"id":"pAeBc9XpJ1uZ","colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"status":"ok","timestamp":1737185193393,"user_tz":-330,"elapsed":23082,"user":{"displayName":"Debasish Bhaskar","userId":"07671455864892136061"}},"outputId":"f22957be-045e-4437-9a8d-ff39416b78fa"},"source":["# Start spark session\n","spark = SparkSession.builder.appName('ETL text data').getOrCreate()\n","spark"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7a82fbcb7290>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://4162ecf23b1d:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.4</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>ETL text data</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"_w1rS1nRJ1ua"},"source":["### Text Analytics"]},{"cell_type":"markdown","metadata":{"id":"QUO-MOywJ1ua"},"source":["#### Get the text data\n","\n","The raw text is from the Gutenberg corpus from the nltk package. Get file ids in Gutenberg corpus."]},{"cell_type":"code","metadata":{"id":"r8JRZaZPJ1ua","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737185194511,"user_tz":-330,"elapsed":1123,"user":{"displayName":"Debasish Bhaskar","userId":"07671455864892136061"}},"outputId":"147e66e2-2d6f-48f3-93c6-c09ffc9522dc"},"source":["nltk.download('gutenberg')\n","\n","# Download dependencies for sent_tokenize()\n","nltk.download('punkt_tab')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/gutenberg.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"bkEWZrVWJ1ua","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737185194511,"user_tz":-330,"elapsed":8,"user":{"displayName":"Debasish Bhaskar","userId":"07671455864892136061"}},"outputId":"5a2efd9d-ed3a-4079-aaea-81a8022fc3a7"},"source":["from nltk.corpus import gutenberg\n","gutenberg_fileids = gutenberg.fileids()\n","gutenberg_fileids"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['austen-emma.txt',\n"," 'austen-persuasion.txt',\n"," 'austen-sense.txt',\n"," 'bible-kjv.txt',\n"," 'blake-poems.txt',\n"," 'bryant-stories.txt',\n"," 'burgess-busterbrown.txt',\n"," 'carroll-alice.txt',\n"," 'chesterton-ball.txt',\n"," 'chesterton-brown.txt',\n"," 'chesterton-thursday.txt',\n"," 'edgeworth-parents.txt',\n"," 'melville-moby_dick.txt',\n"," 'milton-paradise.txt',\n"," 'shakespeare-caesar.txt',\n"," 'shakespeare-hamlet.txt',\n"," 'shakespeare-macbeth.txt',\n"," 'whitman-leaves.txt']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"0_vRcgahJ1ua"},"source":["The file id is `milton-paradise.txt`. Use the nltk.sent_tokenize() function to split text into sentences."]},{"cell_type":"code","metadata":{"id":"XtvBSiuFJ1ub","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737186255241,"user_tz":-330,"elapsed":1510,"user":{"displayName":"Debasish Bhaskar","userId":"07671455864892136061"}},"outputId":"79eb71ae-9d02-49a2-981a-33eac374ae2c"},"source":["milton_paradise = gutenberg.raw('milton-paradise.txt')\n","#print(milton_paradise)\n","pdf = pd.DataFrame({'sentences': nltk.sent_tokenize(milton_paradise)})\n","d = spark.createDataFrame(pdf)\n","d.show(1, truncate= False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|sentences                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n","+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|[Paradise Lost by John Milton 1667] \\n \\n \\nBook I \\n \\n \\nOf Man's first disobedience, and the fruit \\nOf that forbidden tree whose mortal taste \\nBrought death into the World, and all our woe, \\nWith loss of Eden, till one greater Man \\nRestore us, and regain the blissful seat, \\nSing, Heavenly Muse, that, on the secret top \\nOf Oreb, or of Sinai, didst inspire \\nThat shepherd who first taught the chosen seed \\nIn the beginning how the heavens and earth \\nRose out of Chaos: or, if Sion hill \\nDelight thee more, and Siloa's brook that flowed \\nFast by the oracle of God, I thence \\nInvoke thy aid to my adventurous song, \\nThat with no middle flight intends to soar \\nAbove th' Aonian mount, while it pursues \\nThings unattempted yet in prose or rhyme.|\n","+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","only showing top 1 row\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"1wiCzYxgJ1ub"},"source":["From above it can be seen that empty spaces are present in the data."]},{"cell_type":"markdown","metadata":{"id":"EW2NfTKfJ1ub"},"source":["#### Transform Data\n","\n","* Remove trailing spaces"]},{"cell_type":"code","source":["# Transform data\n","d_x = d.withColumn(\"sentences\", regexp_replace(col(\"sentences\"), \"\\\\n+\",\"\"))\n","d_x.show(5, truncate= False)"],"metadata":{"id":"jXjsRQ0iant0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform data\n","d1 = d.withColumn(\"sentences\", regexp_replace(col(\"sentences\"), \"\\\\s+\",\" \"))       # replace all spaces with with one space\n","d1 = d1.withColumn(\"sentences\", trim(col(\"sentences\")))                            # remove trailing spaces [\"   Spark\", \"Spark  \", \" Spark\"]\n","d1.show(5, truncate= False)"],"metadata":{"id":"ycki4Xe_a2D0"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iearnIOYJ1ub"},"source":["# Transform data\n","# d1 = d.withColumn(\"sentences\", regexp_replace(col(\"sentences\"), \"\\\\s+\",\"_\"))       # replace all spaces with underscore\n","# d1 = d1.withColumn(\"sentences\", regexp_replace(col(\"sentences\"), \"_\",\" \"))         # replace all underscores with one space\n","# d1 = d1.withColumn(\"sentences\", trim(col(\"sentences\")))                            # remove trailing spaces"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hVrcNODJ1ub","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737185221221,"user_tz":-330,"elapsed":1563,"user":{"displayName":"Debasish Bhaskar","userId":"07671455864892136061"}},"outputId":"7cb508db-9576-4350-ca61-966b1fb91efe"},"source":["d1.show(5, truncate= False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|sentences                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|[Paradise Lost by John Milton 1667] Book I Of Man's first disobedience, and the fruit Of that forbidden tree whose mortal taste Brought death into the World, and all our woe, With loss of Eden, till one greater Man Restore us, and regain the blissful seat, Sing, Heavenly Muse, that, on the secret top Of Oreb, or of Sinai, didst inspire That shepherd who first taught the chosen seed In the beginning how the heavens and earth Rose out of Chaos: or, if Sion hill Delight thee more, and Siloa's brook that flowed Fast by the oracle of God, I thence Invoke thy aid to my adventurous song, That with no middle flight intends to soar Above th' Aonian mount, while it pursues Things unattempted yet in prose or rhyme.|\n","|And chiefly thou, O Spirit, that dost prefer Before all temples th' upright heart and pure, Instruct me, for thou know'st; thou from the first Wast present, and, with mighty wings outspread, Dove-like sat'st brooding on the vast Abyss, And mad'st it pregnant: what in me is dark Illumine, what is low raise and support; That, to the height of this great argument, I may assert Eternal Providence, And justify the ways of God to men.                                                                                                                                                                                                                                                                                         |\n","|Say first--for Heaven hides nothing from thy view, Nor the deep tract of Hell--say first what cause Moved our grand parents, in that happy state, Favoured of Heaven so highly, to fall off From their Creator, and transgress his will For one restraint, lords of the World besides.                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n","|Who first seduced them to that foul revolt?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n","|Th' infernal Serpent; he it was whose guile, Stirred up with envy and revenge, deceived The mother of mankind, what time his pride Had cast him out from Heaven, with all his host Of rebel Angels, by whose aid, aspiring To set himself in glory above his peers, He trusted to have equalled the Most High, If he opposed, and with ambitious aim Against the throne and monarchy of God, Raised impious war in Heaven and battle proud, With vain attempt.                                                                                                                                                                                                                                                                           |\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"yrO3vZ67J1uc"},"source":["# Check for empty lines\n","d1.where(col(\"sentences\")==\"\").count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUTbYzXSJ1uc"},"source":["##### Word Tokenization\n","\n","It is the process of breaking down a paragraph, a sentence or a complete text corpus into an array of words."]},{"cell_type":"code","metadata":{"id":"5TaiZDtkJ1uc"},"source":["from nltk.tokenize import word_tokenize\n","\n","word_udf = udf(lambda x: word_tokenize(x), ArrayType(StringType()))\n","d2 = d1.withColumn(\"words\", word_udf(\"sentences\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VX4130tAJ1uc"},"source":["d2.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F4N9SIngJ1uc"},"source":["From above it can be seen that data has punctuations in it.\n","\n","* **Remove punctuation and stopwords**"]},{"cell_type":"code","metadata":{"id":"DxR9eooPJ1uc"},"source":["# Download stopwords\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rRHaCN0J1ud"},"source":["from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","print(stop_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vSSCh2QeJ1ud"},"source":["\n","punctuation = string.punctuation\n","print(punctuation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gKHU3-bJ1ud"},"source":["# Transform data\n","punct_udf = udf(lambda x: [w for w in x if not w.lower() in punctuation if not w.lower() in stop_words])\n","d3 = d2.withColumn(\"words\", punct_udf(\"words\"))\n","d3.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7CN5QwlJ1ud"},"source":["# Convert dataframe column to arraytype for further processing\n","\n","array_udf = udf(lambda x: x, ArrayType(StringType()))\n","d4 = d3.withColumn(\"words\", array_udf(\"words\"))\n","d4.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0HgsKRUDJ1ud"},"source":["##### Ngrams and collocations\n","\n","Collocation is a sequence of words that occur together unusually often.\n","Bigrams: word pairs. High frequent bigrams are collocations.\n","\n","Let's see how we transform texts to 2-grams, 3-grams, and 4-grams collocations."]},{"cell_type":"code","metadata":{"id":"oSeoJeoMJ1ue"},"source":["ngrams = [NGram(n=n, inputCol='words', outputCol=str(n)+'-grams') for n in [2,3,4]]\n","\n","# build pipeline model\n","pipeline = Pipeline(stages=ngrams)\n","\n","# transform data\n","texts_ngrams = pipeline.fit(d4).transform(d4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxhhYXXrJ1ue"},"source":["# display result\n","texts_ngrams.select('2-grams').show(6, truncate=False)\n","texts_ngrams.select('3-grams').show(6, truncate=False)\n","texts_ngrams.select('4-grams').show(6, truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BozJZvZVJ1ue"},"source":["* Add the number of words column"]},{"cell_type":"code","metadata":{"id":"gnW-5PvwJ1ue"},"source":["# Transform data\n","len_udf = udf(lambda x: len(x), IntegerType())\n","\n","d5 = d4.withColumn(\"no_of_words\", len_udf(\"words\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byxmFkQxJ1ue"},"source":["d5.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iw8-o6gMJ1ue"},"source":["##### **POS (part-of-speech) tagging**\n","\n","It is a process of converting a sentence to forms – list of words, list of tuples (where each tuple is having a form (word, tag)). The tag is a part-of-speech tag and signifies whether the word is a noun, adjective, verb, and so on.\n","\n","To know more about POS tagging click [here](https://cdn.exec.talentsprint.com/static/cds/content/M7_AST5_pos.pdf)."]},{"cell_type":"code","metadata":{"id":"U8JhY1RqJ1uf"},"source":["# Download dependencies for pos_tag()\n","nltk.download('averaged_perceptron_tagger_eng')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2M-3UpFRJ1uf"},"source":["## define schema for returned result from the udf function\n","## the returned result is a list of tuples\n","schema = ArrayType(StructType([\n","            StructField('f1', StringType()),\n","            StructField('f2', StringType())    ]))\n","\n","sent_to_tag_words_udf = udf(lambda x: nltk.pos_tag(x), schema)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PkckMu1MJ1uf"},"source":["# Transform data\n","d6 = d5.withColumn(\"tagged_words\", sent_to_tag_words_udf(\"words\"))\n","d6.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1EYWgCKWJ1uf"},"source":["##### **Frequency Distribution Plot**\n","\n","It gives us information about the number of times a word has occurred within a sentence."]},{"cell_type":"code","metadata":{"id":"NWhKkH9-J1uf"},"source":["from nltk.probability import FreqDist\n","\n","row = d6.select('words').toPandas().iloc[0,0]\n","fd = FreqDist(row)\n","fd.plot(30, cumulative= False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36AvWojEJ1uf"},"source":["From the above plot it can be seen that in the first row, the word 'Man' has occurred twice."]},{"cell_type":"markdown","metadata":{"id":"IwWhqb88J1uf"},"source":["##### **Chunking**\n","Chunking is the process of grouping similar words together based on the nature of the word. It is the process of segmenting and labeling multitokens. Let's see how to do a noun phrase chunking on the tagged words data frame from the previous step.\n","\n","First we need to define a udf function that chunks noun phrases from a list of pos-tagged words."]},{"cell_type":"code","metadata":{"id":"g08SQc6XJ1ug"},"source":["# define a udf function to chunk noun phrases from pos-tagged words\n","grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n","chunk_parser = nltk.RegexpParser(grammar)\n","chunk_parser_udf = udf(lambda x: str(chunk_parser.parse(x)), StringType())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PrAbDbOGJ1ug"},"source":["# Transform data\n","d7 = d6.withColumn(\"NP_chunk\", chunk_parser_udf(\"tagged_words\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0bVIAQOJ1uh"},"source":["d7.select('NP_chunk').show(1, truncate= False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to pretty-print chunks\n","def pretty_print_tree(chunk):\n","    try:\n","        tree = Tree.fromstring(chunk)  # Parse the chunk as an NLTK tree\n","        return tree.pformat(margin=200)  # Pretty print with a margin\n","    except Exception as e:\n","        return str(e)\n","\n","# UDF to apply pretty-printing\n","pretty_print_udf = udf(pretty_print_tree, StringType())\n","\n","# Apply UDF to create a new column with pretty-printed trees\n","d7_pretty = d7.withColumn(\"Pretty_Tree\", pretty_print_udf(d7[\"NP_chunk\"]))\n","\n","row=d7_pretty.collect()[0]\n","print(row['Pretty_Tree'])"],"metadata":{"id":"ScQ-qGYhbC3k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5l8mSsydJ1uh"},"source":["#### Load data"]},{"cell_type":"markdown","metadata":{"id":"8wG_GKthJ1uh"},"source":["**Use Parquet file to store data**"]},{"cell_type":"code","metadata":{"id":"Jwz7yHbcJ1uh"},"source":["d7.write.format(\"parquet\").mode(\"overwrite\").save(\"transformed_text_parquet_data\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vn8a8aVKJ1uh"},"source":["**Read data from Parquet file**"]},{"cell_type":"code","metadata":{"id":"zU67l4gUJ1uh"},"source":["df_text_parquet = spark.read.format(\"parquet\").load(\"transformed_text_parquet_data\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yonfCe5wJ1ui"},"source":["df_text_parquet.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IsSrVl6bJ1ui"},"source":["**Store the data as a `json file`**"]},{"cell_type":"code","metadata":{"id":"B5Jobed2J1ui"},"source":["d7.write.format(\"json\").mode(\"overwrite\").save('transformed_text_json_data.json')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IxhbRkgOJ1ui"},"source":["**Read data from `json` to spark dataframe**"]},{"cell_type":"code","metadata":{"id":"IXldw5HBJ1ui"},"source":["df_text_json = spark.read.format(\"json\").load('transformed_text_json_data.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jcb-LBjAJ1ui"},"source":["df_text_json.show(5)"],"execution_count":null,"outputs":[]}]}