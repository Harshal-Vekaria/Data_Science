{"cells":[{"cell_type":"markdown","metadata":{"id":"hNgLag1Euy3H"},"source":["### AST 1: Fine-tune GPT2"]},{"cell_type":"markdown","metadata":{"id":"-tdtrlAhvIHY"},"source":["## Learning Objectives\n","\n","At the end of the experiment, you will be able to:\n","\n","* load and pre-process data from text file\n","* load and use a pre-trained tokenizer\n","* finetune a GPT-2 language model from Hugging Face's `transformers` library"]},{"cell_type":"markdown","metadata":{"id":"2nmP8OaXuO--"},"source":["## Dataset Description\n","\n","The text data file is taken from one of the Project Gutenberg's eBooks named \"***The Buddha's Path of Virtue: A Translation of the Dhammapada*** by F. L. Woodward\", refer [here](https://www.gutenberg.org/files/35185/35185-h/35185-h.htm).\n","\n","To know more about Project Gutenberg's eBooks, refer [here](https://www.gutenberg.org/)."]},{"cell_type":"markdown","metadata":{"id":"hK3ZixaWfhHD"},"source":["### **GPT-2**\n","\n","In recent years, the OpenAI GPT-2 exhibited an impressive ability to write coherent and passionate essays that exceeded what current language models can produce. The GPT-2 wasn't a particularly novel architecture - its architecture is very similar to the **decoder-only transformer**. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset.\n","\n","Here, we are going to fine-tune the GPT2 model with the text of Project Gutenberg's eBook - The Buddha's Path of Virtue. We can expect that the model will be able to reply to the prompt related to the subject matter of this book after fine-tuning.\n","\n","To know more about GPT-2, refer [here](http://jalammar.github.io/illustrated-gpt2/)."]},{"cell_type":"markdown","metadata":{"id":"rKQ0Fvl_jNqU"},"source":["### Setup Steps:"]},{"cell_type":"markdown","metadata":{"id":"9RH8Ecq9sbYU"},"source":["### Importing required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JfqnHAnMYfWF"},"outputs":[],"source":["import os\n","import re\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["#@title Download Dataset\n","!wget -qq 'https://cdn.exec.talentsprint.com/static/cds/content/35185-0.txt'\n","print(\"Dataset Downloaded Successfully..\")"],"metadata":{"cellView":"form","id":"2TRwNbIYGOam"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ChIKT30jKuo-"},"source":["### Load the data"]},{"cell_type":"markdown","metadata":{"id":"a_Bn7925nhYk"},"source":["The data is in a text file (.txt)\n","\n","Create functions to read text files:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjhi8kpfYdlV"},"outputs":[],"source":["# Functions to read different file types\n","\n","def read_txt(file_path):\n","    with open(file_path, \"r\") as file:\n","        text = file.read()\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyMCdjk7bn3f"},"outputs":[],"source":["# Read files/documents\n","\n","file_path = '/content/35185-0.txt'\n","text_file = read_txt(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHEHx_HTc9Bf"},"outputs":[],"source":["print(text_file)"]},{"cell_type":"markdown","metadata":{"id":"9ESwtGMaL5vJ"},"source":["### Pre-processing\n","\n","- Remove any excess newline characters from the text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tYqGszOL-9f"},"outputs":[],"source":["# Remove excess newline characters\n","text_file = re.sub(r'\\n+', '\\n', text_file).strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZleqLz-jdYU2"},"outputs":[],"source":["print(text_file)"]},{"cell_type":"markdown","metadata":{"id":"DUO4loorLsA9"},"source":["### Split the text into training and validation sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Au9dHz2IcOLa"},"outputs":[],"source":["# Split the text into training and validation sets\n","\n","train_fraction = 0.8\n","split_index = int(train_fraction * len(text_file))\n","\n","train_text = text_file[:split_index]\n","val_text = text_file[split_index:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_7YIqYzIawcO"},"outputs":[],"source":["len(train_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABHKmK7ZcUlA"},"outputs":[],"source":["# Save the training and validation data as text files\n","\n","with open(\"train.txt\", \"w\") as f:\n","    f.write(train_text)\n","\n","with open(\"val.txt\", \"w\") as f:\n","    f.write(val_text)"]},{"cell_type":"markdown","metadata":{"id":"Y1hlYtjjML1r"},"source":["### Load pre-trained tokenizer - GP2Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"6LIwiw1qnGIc"},"source":["The GPT2Tokenizer is based on ***Byte-Pair-Encoding***.\n","\n","Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model.\n","\n","In BPE, new tokens are added until the desired vocabulary size is reached by learning ***merges***, which are rules to merge two elements of the existing vocabulary together into a new one.\n","\n","Below figure shows how the vocabulary updates as the BPE algorithm progresses.\n","\n","<br>\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Byte-pair-encoding.png\" width=450px>\n","</center>\n","\n","To know more about Byte-Pair Encoding, refer [here](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt#byte-pair-encoding-tokenization).\n","\n","<br>\n","\n","Some of the parameters required to create a GP2Tokenizer includes:\n","\n","- ***vocab_file (str):*** path to the vocabulary json file; maps token to integer ids\n","\n","- ***merges_file (str):*** path to the ***merges*** file; contains the merge rule; The merge rule file should have one merge rule per line. Every merge rule contains merge entities separated by a space.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HXxBS2ShDIYH"},"source":["Here, we will instantiate a GPT-2 tokenizer from a predefined tokenizer using `from_pretrained()` method.\n","\n","It includes a parameter:\n","\n","- ***pretrained_model_name_or_path:*** It can be a string of a predefined tokenizer hosted inside a model repo on huggingface.co.\n","\n","    For example: *gpt2, gpt2-medium, gpt2-large, or gpt2-xl*\n","\n","    This will download the corresponding vocab, merges, and config files."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qiMe9TAplyj"},"outputs":[],"source":["# Set up the tokenizer\n","checkpoint = \"gpt2\"\n","tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)    # also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_l2MihgfwEPh"},"outputs":[],"source":["# Tokenize sample text using GP2Tokenizer\n","sample_ids = tokenizer(\"Hello world\")\n","sample_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXYqWSzLwaBA"},"outputs":[],"source":["# Generate tokens for sample text\n","sample_tokens = tokenizer.convert_ids_to_tokens(sample_ids['input_ids'])\n","sample_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sHuZKceCwNKm"},"outputs":[],"source":["# Generate original text back\n","tokenizer.convert_tokens_to_string(sample_tokens)"]},{"cell_type":"markdown","metadata":{"id":"InX4FOvgP0mi"},"source":["### Tokenize text data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crjMEbLfVOpq"},"outputs":[],"source":["# Tokenize train text\n","train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"train.txt\", block_size=128)\n","\n","# Tokenize validation text\n","val_dataset = TextDataset(tokenizer=tokenizer, file_path=\"val.txt\", block_size=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JstT8I-BW0Pd"},"outputs":[],"source":["# Length of train and validation set\n","len(train_dataset), len(val_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_u3AKDcZfnTi"},"outputs":[],"source":["# Batch-size\n","train_dataset[0].shape, val_dataset[0].shape"]},{"cell_type":"markdown","metadata":{"id":"itcSXbZMib_Y"},"source":["### Data Collator\n","\n","Data collators are objects that:\n","\n","- will form a batch by using a list of dataset elements as input\n","- may apply some processing (like padding)\n","\n","One of the data collators, `DataCollatorForLanguageModeling`, can also apply some random data augmentation (like random masking) on the formed batch.\n","\n","<br>\n","\n","`DataCollatorForLanguageModeling` is a data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they are not all of the same length.\n","\n","Parameters:\n","\n","- ***tokenizer:*** The tokenizer used for encoding the data.\n","- ***mlm*** (bool, optional, default=True): Whether or not to use masked language modeling.\n","    - If set to False, the labels are the same as the inputs with the padding tokens ignored (by setting them to -100).\n","    - Otherwise, the labels are -100 for non-masked tokens and the value to predict for the masked token.\n","- ***return_tensors*** (str): The type of Tensor to return. Allowable values are “np”, “pt” and “tf” for numpy array, pytorch tensor, and tensorflow tensor respectively.\n","\n","To know more about `DataCollatorForLanguageModeling` parameters, refer [here](https://huggingface.co/docs/transformers/v4.32.0/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_XWmIF3cmhU"},"outputs":[],"source":["# Create a Data collator object\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\")"]},{"cell_type":"markdown","metadata":{"id":"uophCXjYq9MO"},"source":["### Load pre-trained Model"]},{"cell_type":"markdown","metadata":{"id":"nuE_NdXuqvw0"},"source":["***GPT2LMHeadModel*** is the GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n","\n","This model is a PyTorch `torch.nn.Module` subclass which can be used as a regular PyTorch Module.\n","\n","Parameters:\n","\n","- ***config (GPT2Config):*** Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration.\n","\n","Here, we will instantiate a pretrained pytorch model from a pre-trained model configuration, using `from_pretrained()` method, that will load the weights associated with the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxQWgssCqy7j"},"outputs":[],"source":["# Set up the model\n","model = GPT2LMHeadModel.from_pretrained(checkpoint)    # also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl"]},{"cell_type":"markdown","metadata":{"id":"pM13pdhzJY8y"},"source":["**Note: The training time for different GPT models with GPU for this dataset are as follows:**\n","\n","* **GPT-2 : ~20 minutes for 100 epochs**\n","\n","* **GPT-2 Medium:  ~1 hour for 100 epochs**\n","\n","* **GPT-2 Large : Run out of memory**"]},{"cell_type":"markdown","metadata":{"id":"LdcpMx9QOPnU"},"source":["### Fine-tune Model"]},{"cell_type":"markdown","metadata":{"id":"D1LqRO_Unfmk"},"source":["Train a GPT-2 model using the provided training arguments. Save the resulting trained model and tokenizer to a specified output directory."]},{"cell_type":"markdown","metadata":{"id":"J7pZfFvsopWT"},"source":["The `Trainer` class provides an API for feature-complete training in PyTorch for most standard use cases.\n","\n","Before instantiating your Trainer, create a `TrainingArguments` to access all the points of customization during training.\n","\n","`TrainingArguments` parameters:\n","\n","- ***output_dir*** (str): The output directory where the model predictions and checkpoints will be written.\n","- ***overwrite_output_dir*** (bool, optional, default=False): If True, overwrite the content of the output directory. Use this to continue training if output_dir points to a checkpoint directory.\n","- ***per_device_train_batch_size*** (int, optional, default=8): The batch size per GPU/TPU/MPS/NPU core/CPU for training.\n","- ***per_device_eval_batch_size*** (int, optional, default=8): The batch size per GPU/TPU/MPS/NPU core/CPU for evaluation.\n","- ***save_total_limit*** (int, optional): If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.\n","\n","To know more about `TrainingArguments` parameters, refer [here](https://huggingface.co/docs/transformers/v4.32.0/en/main_classes/trainer#transformers.TrainingArguments).\n","\n","To know more about `Trainer` parameters, refer [here](https://huggingface.co/docs/transformers/v4.32.0/en/main_classes/trainer#transformers.Trainer)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPZiEvn2cuPR"},"outputs":[],"source":["# Set up the training arguments\n","\n","model_output_path = \"/content/gpt_model\"\n","\n","training_args = TrainingArguments(\n","    output_dir = model_output_path,\n","    overwrite_output_dir = True,\n","    per_device_train_batch_size = 4, # try with 2\n","    per_device_eval_batch_size = 4,  #  try with 2\n","    num_train_epochs = 100,\n","    save_steps = 1_000,\n","    save_total_limit = 2,\n","    logging_dir = './logs',\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UX-EmL_dc_H7"},"outputs":[],"source":["# Train the model\n","trainer = Trainer(\n","    model = model,\n","    args = training_args,\n","    data_collator = data_collator,\n","    train_dataset = train_dataset,\n","    eval_dataset = val_dataset,\n",")\n","\n","trainer.train()\n","\n","# Save the model\n","trainer.save_model(model_output_path)\n","\n","# Save the tokenizer\n","tokenizer.save_pretrained(model_output_path)"]},{"cell_type":"markdown","metadata":{"id":"4izo_-go8cDP"},"source":["### Test Model with user input prompts"]},{"cell_type":"markdown","metadata":{"id":"m0JzfOFRcUDI"},"source":["##### Now, let us test the model with some prompt\n"]},{"cell_type":"markdown","metadata":{"id":"W8WnPwpHnz57"},"source":["The `generate_response()` function takes a trained *model*, *tokenizer*, and a *prompt* string as input and generates a response using the GPT-2 model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qeTKPArfgDJW"},"outputs":[],"source":["def generate_response(model, tokenizer, prompt, max_length=100):\n","\n","    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")      # 'pt' for returning pytorch tensor\n","\n","    # Create the attention mask and pad token id\n","    attention_mask = torch.ones_like(input_ids)\n","    pad_token_id = tokenizer.eos_token_id\n","\n","    output = model.generate(\n","        input_ids,\n","        max_length=max_length,\n","        num_return_sequences=1,\n","        attention_mask=attention_mask,\n","        pad_token_id=pad_token_id\n","    )\n","\n","    return tokenizer.decode(output[0], skip_special_tokens=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJJ3bzD9fsJv"},"outputs":[],"source":["# Load the fine-tuned model and tokenizer\n","\n","my_model = GPT2LMHeadModel.from_pretrained(model_output_path)\n","my_tokenizer = GPT2Tokenizer.from_pretrained(model_output_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIpKkHVVf9Ly"},"outputs":[],"source":["# Testing with given prompt 1\n","\n","prompt = \"What is teaching of Buddha?\"  # Replace with your desired prompt\n","response = generate_response(my_model, my_tokenizer, prompt)\n","print(\"Generated response:\", response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBlBeSY7gDyO"},"outputs":[],"source":["# Testing with given prompt 2\n","prompt = \"what is dharma ?\"  # Replace with your desired prompt\n","response = generate_response(my_model, my_tokenizer, prompt, max_length=150)\n","print(\"Generated response:\", response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVYPlTfbvS7z"},"outputs":[],"source":["# Testing with given prompt 3\n","\n","prompt = \"how to live ?\"  # Replace with your desired prompt\n","response = generate_response(my_model, my_tokenizer, prompt, max_length=150)\n","print(\"Generated response:\", response)"]},{"cell_type":"markdown","metadata":{"id":"XHcQBzdJeB27"},"source":["In the case of the GPT-2 tokenizer, the model uses a byte-pair encoding (BPE) algorithm, which tokenizes text into subword units. As a result, one word might be represented by multiple tokens.\n","\n","For example, if you set max_length to 50, the generated response will be limited to 50 tokens, which could be fewer than 50 words, depending on the text."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}